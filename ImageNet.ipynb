{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zABk9_Avcyp_"
   },
   "source": [
    "#Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib scipy opencv-python pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install unzip\n",
    "!apt-get install ffmpeg libsm6 libxext6  -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://www.dropbox.com/s/l0y7cqqymjef2sh/tiny-imagenet-200.zip?dl=0'\n",
    "!mv 'tiny-imagenet-200.zip?dl=0' 'ImageNetData.zip'\n",
    "!unzip 'ImageNetData.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet.ipynb\t  imagenet  onstart.log  ports.log\r\n",
      "ImageNetData.zip  models    onstart.sh\t utils.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNet(Dataset):\n",
    "    def __init__(self):\n",
    "        self.imgs_path = \"imagenet/\"\n",
    "        sub_folders = glob.glob(self.imgs_path + \"*\")\n",
    "        self.data = []\n",
    "        self.classes = []\n",
    "        for sub_folder in sub_folders:\n",
    "            file_list = glob.glob(sub_folder + \"/*\")\n",
    "            for class_path in file_list:\n",
    "                class_name = class_path.split(\"/\")[-1]\n",
    "                for img_path in glob.glob(class_path + \"/*.JPEG\"):\n",
    "                    self.data.append([img_path, class_name])\n",
    "                    self.classes.append(class_name)\n",
    "        self.classes = list(set(self.classes))\n",
    "        self.class_map = dict()\n",
    "        for i in range(len(self.classes)):\n",
    "            self.class_map[self.classes[i]] = i\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_name = self.data[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        class_id = self.class_map[class_name]\n",
    "        img_tensor = torch.from_numpy(img).float()\n",
    "        img_tensor = img_tensor.permute(2, 0, 1)\n",
    "        class_id = torch.tensor(class_id)\n",
    "        return img_tensor, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176,
     "referenced_widgets": [
      "48e26e6e1e01429d88a7a002590d804b",
      "5981bee7f92940b5823aed3556f79b0e",
      "28b6c0c9832d41b68bdcf529a88e9823",
      "b4b284eeaee949968c2ed52c4737227b",
      "d8fd8e6dbdc64aa383222481d2491ba8",
      "a75fb9d528b74555ac013f55593ddd6b",
      "705e9f12834c4fbca2f3fa8d304bad23",
      "c9142c841ae147339da419a60e489e62",
      "90afe6ae2ba641828ed88bbca6181d2d",
      "95bdb61438494daf8297d3481a759e64",
      "b482a3fbf44d4c08a3bae6e80bdc1fca"
     ]
    },
    "id": "_705tOlI1f1r",
    "outputId": "531234d3-3639-4d0c-ed2d-9f662d188457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images-size: torch.Size([32, 3, 64, 64])\n",
      "out-size: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "from torchvision import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from models import *\n",
    "from utils import progress_bar\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "transforms = transforms.Compose(\n",
    "[\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = ImageNet()\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "images, labels = next(iter(train_dataloader)) \n",
    "print(\"images-size:\", images.shape)\n",
    "\n",
    "out = torchvision.utils.make_grid(images)\n",
    "print(\"out-size:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if train_loss < self.min_delta:\n",
    "            self.min_delta -= self.min_delta*0.5\n",
    "            \n",
    "        if (validation_loss - train_loss) > self.min_delta:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.tolerance:  \n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYiFRWaBc7NO"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Z_pR1aYacawp"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(train_dataloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return train_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HV2oF63c8mf"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "w0VRvtr_ccQo"
   },
   "outputs": [],
   "source": [
    "def test(epoch,model_name):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(test_dataloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/'+model_name+'.pth')\n",
    "        best_acc = acc\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "5hbLSbfys3K5"
   },
   "outputs": [],
   "source": [
    "classes = dataset.classes\n",
    "lr = 0.01\n",
    "n_epochs = 20\n",
    "in_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Fa9Z6-1scZFv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " [===>.............................................................]  Step: 122ms | Tot: 14s915ms | Loss: 5.402 | Acc: 0.835% (35/419 131/2750 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_429/3335175057.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DenseNet'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_429/3264958514.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 454\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "net = densenet_cifar(in_dim=in_dim,num_classes=len(classes))\n",
    "net = net.to(device)\n",
    "if device == 'cuda:0':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "early_stopping = EarlyStopping(tolerance=5, min_delta=1)\n",
    "\n",
    "epoch = 0\n",
    "while(not(early_stopping.early_stop)):\n",
    "    train_loss = train(epoch) / len(train_dataloader)\n",
    "    validation_loss = test(epoch,'DenseNet') / len(test_dataloader)\n",
    "    scheduler.step()\n",
    "    early_stopping(train_loss,validation_loss)\n",
    "    epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6wck4o-Fsj9y",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " [================================================================>]  Step: 39ms | Tot: 1m51s | Loss: 4.251 | Acc: 11.193% (9850/8800 2750/2750   =>...........................................................]  Step: 41ms | Tot: 9s166ms | Loss: 5.239 | Acc: 2.142% (157/732 229/2750 =======>.........................................................]  Step: 40ms | Tot: 13s320ms | Loss: 5.140 | Acc: 2.672% (283/1059 331/2750 ==========>......................................................]  Step: 41ms | Tot: 17s388ms | Loss: 5.069 | Acc: 2.973% (410/1379 431/2750 ==========>......................................................]  Step: 41ms | Tot: 17s794ms | Loss: 5.062 | Acc: 3.005% (424/1411 441/2750 ===========>.....................................................]  Step: 41ms | Tot: 19s891ms | Loss: 5.031 | Acc: 3.264% (515/1577 493/2750 ============>....................................................]  Step: 42ms | Tot: 21s141ms | Loss: 5.012 | Acc: 3.375% (566/1676 524/2750 =============>...................................................]  Step: 41ms | Tot: 22s281ms | Loss: 4.997 | Acc: 3.431% (606/1766 552/2750 ==============>..................................................]  Step: 41ms | Tot: 24s29ms | Loss: 4.972 | Acc: 3.598% (685/1904 595/2750 ===============>.................................................]  Step: 40ms | Tot: 26s186ms | Loss: 4.941 | Acc: 3.756% (780/2076 649/2750 ================>................................................]  Step: 40ms | Tot: 27s603ms | Loss: 4.919 | Acc: 3.906% (855/2188 684/2750 ====================>............................................]  Step: 41ms | Tot: 35s441ms | Loss: 4.821 | Acc: 4.841% (1360/2809 878/2750 ======================>..........................................]  Step: 41ms | Tot: 39s38ms | Loss: 4.782 | Acc: 5.112% (1582/3094 967/2750 =======================>.........................................]  Step: 41ms | Tot: 39s563ms | Loss: 4.776 | Acc: 5.166% (1620/3136 980/2750 =======================>.........................................]  Step: 40ms | Tot: 40s425ms | Loss: 4.767 | Acc: 5.273% (1689/3203 1001/2750 =========================>.......................................]  Step: 41ms | Tot: 43s336ms | Loss: 4.734 | Acc: 5.624% (1931/3433 1073/2750 ============================>....................................]  Step: 41ms | Tot: 49s416ms | Loss: 4.671 | Acc: 6.230% (2440/3916 1224/2750 =============================>...................................]  Step: 41ms | Tot: 50s102ms | Loss: 4.665 | Acc: 6.283% (2495/3971 1241/2750 =================================>...............................]  Step: 41ms | Tot: 56s639ms | Loss: 4.607 | Acc: 6.912% (3103/4489 1403/2750 ===================================>.............................]  Step: 41ms | Tot: 59s958ms | Loss: 4.580 | Acc: 7.212% (3427/4752 1485/2750 ===================================>.............................]  Step: 41ms | Tot: 1m363ms | Loss: 4.577 | Acc: 7.239% (3463/4784 1495/2750 ===================================>.............................]  Step: 41ms | Tot: 1m1s | Loss: 4.572 | Acc: 7.308% (3538/4841 1513/2750 =====================================>...........................]  Step: 41ms | Tot: 1m3s | Loss: 4.555 | Acc: 7.515% (3778/5027 1571/2750 ===========================================>.....................]  Step: 40ms | Tot: 1m13s | Loss: 4.476 | Acc: 8.481% (4964/5852 1829/2750 ================================================>................]  Step: 41ms | Tot: 1m23s | Loss: 4.412 | Acc: 9.201% (6083/6611 2066/2750 ==================================================>..............]  Step: 40ms | Tot: 1m25s | Loss: 4.396 | Acc: 9.374% (6374/6800 2125/2750 ====================================================>............]  Step: 41ms | Tot: 1m28s | Loss: 4.379 | Acc: 9.541% (6720/7043 2201/2750 ======================================================>..........]  Step: 41ms | Tot: 1m32s | Loss: 4.359 | Acc: 9.790% (7165/7318 2287/2750 ========================================================>........]  Step: 41ms | Tot: 1m37s | Loss: 4.330 | Acc: 10.172% (7822/7689 2403/2750 =========================================================>.......]  Step: 40ms | Tot: 1m37s | Loss: 4.328 | Acc: 10.197% (7874/7721 2413/2750 ===========================================================>.....]  Step: 41ms | Tot: 1m41s | Loss: 4.307 | Acc: 10.458% (8370/8003 2501/2750 ===========================================================>.....]  Step: 41ms | Tot: 1m42s | Loss: 4.300 | Acc: 10.556% (8556/8105 2533/2750 ============================================================>....]  Step: 40ms | Tot: 1m42s | Loss: 4.297 | Acc: 10.583% (8632/8156 2549/2750 =============================================================>...]  Step: 41ms | Tot: 1m45s | Loss: 4.283 | Acc: 10.762% (8961/8326 2602/2750 =============================================================>...]  Step: 41ms | Tot: 1m45s | Loss: 4.280 | Acc: 10.811% (9064/8384 2620/2750 ==============================================================>..]  Step: 40ms | Tot: 1m46s | Loss: 4.276 | Acc: 10.875% (9170/8432 2635/2750 ================================================================>]  Step: 41ms | Tot: 1m50s | Loss: 4.253 | Acc: 11.169% (9786/8761 2738/2750 \n",
      " [================================================================>]  Step: 26ms | Tot: 12s229ms | Loss: 3.795 | Acc: 17.686% (3891/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [================================================================>]  Step: 40ms | Tot: 1m50s | Loss: 3.367 | Acc: 24.003% (21123/8800 2750/2750 0  =============>.................................................]  Step: 41ms | Tot: 26s639ms | Loss: 3.526 | Acc: 21.360% (4525/2118 662/2750 ================>................................................]  Step: 41ms | Tot: 27s882ms | Loss: 3.521 | Acc: 21.483% (4764/2217 693/2750 =========================>.......................................]  Step: 41ms | Tot: 43s833ms | Loss: 3.495 | Acc: 21.973% (7629/3472 1085/2750 ===========================>.....................................]  Step: 41ms | Tot: 46s786ms | Loss: 3.492 | Acc: 22.037% (8166/3705 1158/2750 ============================>....................................]  Step: 41ms | Tot: 48s974ms | Loss: 3.487 | Acc: 22.125% (8581/3878 1212/2750 ==============================>..................................]  Step: 41ms | Tot: 51s640ms | Loss: 3.480 | Acc: 22.298% (9119/4089 1278/2750 ==============================>..................................]  Step: 41ms | Tot: 52s935ms | Loss: 3.477 | Acc: 22.302% (9349/4192 1310/2750 ===================================>.............................]  Step: 40ms | Tot: 1m62ms | Loss: 3.461 | Acc: 22.547% (10707/4748 1484/2750 ===================================>.............................]  Step: 40ms | Tot: 1m948ms | Loss: 3.459 | Acc: 22.566% (10875/4819 1506/2750 ===================================================>.............]  Step: 40ms | Tot: 1m27s | Loss: 3.407 | Acc: 23.348% (16250/6960 2175/2750 =======================================================>.........]  Step: 41ms | Tot: 1m34s | Loss: 3.394 | Acc: 23.562% (17651/7491 2341/2750 =======================================================>.........]  Step: 41ms | Tot: 1m34s | Loss: 3.394 | Acc: 23.583% (17742/7523 2351/2750 ========================================================>........]  Step: 41ms | Tot: 1m37s | Loss: 3.390 | Acc: 23.638% (18207/7702 2407/2750 \n",
      " [================================================================>]  Step: 12ms | Tot: 12s427ms | Loss: 3.670 | Acc: 21.936% (4826/2200 688/688 =======>......................................................]  Step: 17ms | Tot: 2s53ms | Loss: 3.701 | Acc: 22.128% (786/355 111/688 ============================>....................................]  Step: 17ms | Tot: 5s483ms | Loss: 3.684 | Acc: 21.823% (2109/966 302/688 ======================================================>..........]  Step: 20ms | Tot: 10s396ms | Loss: 3.676 | Acc: 21.886% (4020/1836 574/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 40ms | Tot: 1m51s | Loss: 2.943 | Acc: 31.556% (27769/8800 2750/2750 0  ==========>....................................................]  Step: 40ms | Tot: 20s714ms | Loss: 3.022 | Acc: 29.958% (4937/1648 515/2750 ================>................................................]  Step: 40ms | Tot: 28s542ms | Loss: 3.014 | Acc: 30.160% (6833/2265 708/2750 =================>...............................................]  Step: 41ms | Tot: 30s190ms | Loss: 3.018 | Acc: 30.114% (7208/2393 748/2750 =================>...............................................]  Step: 40ms | Tot: 30s683ms | Loss: 3.015 | Acc: 30.148% (7332/2432 760/2750 =================>...............................................]  Step: 40ms | Tot: 30s764ms | Loss: 3.015 | Acc: 30.147% (7351/2438 762/2750 ==================>..............................................]  Step: 41ms | Tot: 30s966ms | Loss: 3.015 | Acc: 30.154% (7401/2454 767/2750 ===================>.............................................]  Step: 40ms | Tot: 32s516ms | Loss: 3.015 | Acc: 30.175% (7773/2576 805/2750 =====================>...........................................]  Step: 41ms | Tot: 37s530ms | Loss: 3.014 | Acc: 30.231% (8987/2972 929/2750 ======================>..........................................]  Step: 41ms | Tot: 37s814ms | Loss: 3.014 | Acc: 30.225% (9053/2995 936/2750 =======================>.........................................]  Step: 41ms | Tot: 39s472ms | Loss: 3.013 | Acc: 30.246% (9456/3126 977/2750 ========================>........................................]  Step: 40ms | Tot: 42s114ms | Loss: 3.010 | Acc: 30.347% (10119/3334 1042/2750 =========================>.......................................]  Step: 41ms | Tot: 43s7ms | Loss: 3.011 | Acc: 30.313% (10321/3404 1064/2750 ============================>....................................]  Step: 41ms | Tot: 49s582ms | Loss: 3.004 | Acc: 30.330% (11899/3923 1226/2750 ==================================>..............................]  Step: 40ms | Tot: 58s837ms | Loss: 3.002 | Acc: 30.447% (14108/4633 1448/2750 ==================================>..............................]  Step: 40ms | Tot: 59s575ms | Loss: 3.001 | Acc: 30.463% (14291/4691 1466/2750 =======================================>.........................]  Step: 40ms | Tot: 1m8s | Loss: 2.991 | Acc: 30.673% (16519/5385 1683/2750 ===================================================>.............]  Step: 40ms | Tot: 1m28s | Loss: 2.968 | Acc: 31.085% (21645/6963 2176/2750 ===============================================================>.]  Step: 40ms | Tot: 1m48s | Loss: 2.947 | Acc: 31.490% (26925/8550 2672/2750 \n",
      " [================================================================>]  Step: 12ms | Tot: 12s57ms | Loss: 2.981 | Acc: 31.409% (6910/2200 688/688  \n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      " [================================================================>]  Step: 42ms | Tot: 1m50s | Loss: 2.654 | Acc: 37.181% (32719/8800 2750/2750 0  >............................................................]  Step: 40ms | Tot: 6s649ms | Loss: 2.695 | Acc: 37.555% (2067/550 172/2750 ====>............................................................]  Step: 40ms | Tot: 8s58ms | Loss: 2.703 | Acc: 37.139% (2472/665 208/2750 ========>........................................................]  Step: 41ms | Tot: 13s872ms | Loss: 2.714 | Acc: 36.665% (4130/1126 352/2750 ============>....................................................]  Step: 40ms | Tot: 21s445ms | Loss: 2.699 | Acc: 36.810% (6349/1724 539/2750 ============>....................................................]  Step: 40ms | Tot: 21s486ms | Loss: 2.699 | Acc: 36.794% (6358/1728 540/2750 =============>...................................................]  Step: 40ms | Tot: 22s250ms | Loss: 2.699 | Acc: 36.734% (6571/1788 559/2750 ===================>.............................................]  Step: 41ms | Tot: 32s163ms | Loss: 2.681 | Acc: 37.066% (9560/2579 806/2750 ===================>.............................................]  Step: 41ms | Tot: 32s487ms | Loss: 2.682 | Acc: 37.032% (9646/2604 814/2750 ====================>............................................]  Step: 41ms | Tot: 35s475ms | Loss: 2.683 | Acc: 37.007% (10516/2841 888/2750 ======================>..........................................]  Step: 41ms | Tot: 37s656ms | Loss: 2.682 | Acc: 37.019% (11159/3014 942/2750 =======================>.........................................]  Step: 40ms | Tot: 39s435ms | Loss: 2.683 | Acc: 36.945% (11657/3155 986/2750 =========================>.......................................]  Step: 41ms | Tot: 42s924ms | Loss: 2.687 | Acc: 36.806% (12626/3430 1072/2750 ==========================>......................................]  Step: 41ms | Tot: 45s224ms | Loss: 2.686 | Acc: 36.782% (13277/3609 1128/2750 ==========================>......................................]  Step: 41ms | Tot: 45s829ms | Loss: 2.686 | Acc: 36.786% (13455/3657 1143/2750 =============================>...................................]  Step: 40ms | Tot: 49s933ms | Loss: 2.685 | Acc: 36.824% (14659/3980 1244/2750 =============================>...................................]  Step: 40ms | Tot: 50s136ms | Loss: 2.684 | Acc: 36.837% (14723/3996 1249/2750 =====================================>...........................]  Step: 41ms | Tot: 1m4s | Loss: 2.679 | Acc: 36.836% (18813/5107 1596/2750 =====================================>...........................]  Step: 40ms | Tot: 1m4s | Loss: 2.679 | Acc: 36.812% (18942/5145 1608/2750 ======================================>..........................]  Step: 41ms | Tot: 1m4s | Loss: 2.679 | Acc: 36.791% (18990/5161 1613/2750 ===========================================>.....................]  Step: 41ms | Tot: 1m14s | Loss: 2.673 | Acc: 36.947% (21896/5926 1852/2750 ==============================================>..................]  Step: 40ms | Tot: 1m19s | Loss: 2.671 | Acc: 36.977% (23417/6332 1979/2750 ==============================================>..................]  Step: 41ms | Tot: 1m19s | Loss: 2.670 | Acc: 36.976% (23511/6358 1987/2750 ===============================================>.................]  Step: 41ms | Tot: 1m21s | Loss: 2.672 | Acc: 36.925% (23998/6499 2031/2750 ================================================>................]  Step: 40ms | Tot: 1m22s | Loss: 2.670 | Acc: 36.926% (24330/6588 2059/2750 ================================================>................]  Step: 40ms | Tot: 1m23s | Loss: 2.669 | Acc: 36.936% (24443/6617 2068/2750 =================================================>...............]  Step: 41ms | Tot: 1m23s | Loss: 2.668 | Acc: 36.934% (24619/6665 2083/2750 =================================================>...............]  Step: 40ms | Tot: 1m25s | Loss: 2.669 | Acc: 36.917% (24997/6771 2116/2750 ==================================================>..............]  Step: 41ms | Tot: 1m25s | Loss: 2.670 | Acc: 36.901% (25081/6796 2124/2750 =====================================================>...........]  Step: 40ms | Tot: 1m30s | Loss: 2.663 | Acc: 37.029% (26625/7190 2247/2750 \n",
      " [================================================================>]  Step: 12ms | Tot: 12s16ms | Loss: 2.790 | Acc: 35.236% (7752/2200 688/688  \n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 39ms | Tot: 1m50s | Loss: 2.408 | Acc: 42.017% (36975/8800 2750/2750 0  ........................................................]  Step: 40ms | Tot: 923ms | Loss: 2.439 | Acc: 41.193% (290/70 22/2750 >................................................................]  Step: 40ms | Tot: 1s652ms | Loss: 2.375 | Acc: 42.109% (539/128 40/2750 ==========>......................................................]  Step: 41ms | Tot: 17s913ms | Loss: 2.392 | Acc: 42.148% (5894/1398 437/2750 ===========>.....................................................]  Step: 41ms | Tot: 19s937ms | Loss: 2.388 | Acc: 42.037% (6551/1558 487/2750 ===============>.................................................]  Step: 40ms | Tot: 26s509ms | Loss: 2.405 | Acc: 41.745% (8683/2080 650/2750 =================>...............................................]  Step: 40ms | Tot: 30s369ms | Loss: 2.402 | Acc: 41.907% (10004/2387 746/2750 ===========================>.....................................]  Step: 41ms | Tot: 47s715ms | Loss: 2.401 | Acc: 41.914% (15800/3769 1178/2750 ============================>....................................]  Step: 40ms | Tot: 49s125ms | Loss: 2.400 | Acc: 41.929% (16275/3881 1213/2750 ============================>....................................]  Step: 40ms | Tot: 49s409ms | Loss: 2.402 | Acc: 41.875% (16348/3904 1220/2750 =============================>...................................]  Step: 40ms | Tot: 51s18ms | Loss: 2.403 | Acc: 41.876% (16871/4028 1259/2750 =====================================>...........................]  Step: 41ms | Tot: 1m4s | Loss: 2.407 | Acc: 41.859% (21231/5072 1585/2750 =======================================================>.........]  Step: 40ms | Tot: 1m34s | Loss: 2.409 | Acc: 41.966% (31317/7462 2332/2750 \n",
      " [================================================================>]  Step: 19ms | Tot: 12s132ms | Loss: 2.701 | Acc: 36.936% (8126/2200 688/688 ..........................................................]  Step: 17ms | Tot: 496ms | Loss: 2.549 | Acc: 38.179% (281/73 23/688 =======>.........................................................]  Step: 20ms | Tot: 1s472ms | Loss: 2.692 | Acc: 36.511% (923/252 79/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      " [================================================================>]  Step: 40ms | Tot: 1m50s | Loss: 2.203 | Acc: 46.066% (40538/8800 2750/2750 0  ==========>....................................................]  Step: 41ms | Tot: 21s422ms | Loss: 2.133 | Acc: 47.347% (8121/1715 536/2750 ===============>.................................................]  Step: 40ms | Tot: 25s880ms | Loss: 2.138 | Acc: 47.286% (9775/2067 646/2750 ===============>.................................................]  Step: 40ms | Tot: 26s446ms | Loss: 2.138 | Acc: 47.296% (9989/2112 660/2750 =================>...............................................]  Step: 40ms | Tot: 29s258ms | Loss: 2.143 | Acc: 47.119% (11022/2339 731/2750 =======================>.........................................]  Step: 41ms | Tot: 40s584ms | Loss: 2.163 | Acc: 46.749% (15169/3244 1014/2750 =========================>.......................................]  Step: 41ms | Tot: 43s602ms | Loss: 2.166 | Acc: 46.720% (16281/3484 1089/2750 ==========================>......................................]  Step: 40ms | Tot: 45s18ms | Loss: 2.169 | Acc: 46.669% (16786/3596 1124/2750 ===========================>.....................................]  Step: 41ms | Tot: 45s908ms | Loss: 2.169 | Acc: 46.654% (17109/3667 1146/2750 ===========================>.....................................]  Step: 40ms | Tot: 47s362ms | Loss: 2.173 | Acc: 46.600% (17626/3782 1182/2750 ==============================>..................................]  Step: 41ms | Tot: 51s928ms | Loss: 2.181 | Acc: 46.412% (19233/4144 1295/2750 ==============================>..................................]  Step: 41ms | Tot: 51s970ms | Loss: 2.180 | Acc: 46.414% (19249/4147 1296/2750 ==============================>..................................]  Step: 41ms | Tot: 52s256ms | Loss: 2.180 | Acc: 46.427% (19358/4169 1303/2750 ===============================>.................................]  Step: 40ms | Tot: 53s880ms | Loss: 2.181 | Acc: 46.400% (19941/4297 1343/2750 =====================================>...........................]  Step: 41ms | Tot: 1m4s | Loss: 2.192 | Acc: 46.258% (23625/5107 1596/2750 =============================================>...................]  Step: 40ms | Tot: 1m17s | Loss: 2.199 | Acc: 46.039% (28419/6172 1929/2750 ===============================================>.................]  Step: 41ms | Tot: 1m20s | Loss: 2.202 | Acc: 45.988% (29565/6428 2009/2750 ===============================================>.................]  Step: 41ms | Tot: 1m21s | Loss: 2.202 | Acc: 45.998% (29777/6473 2023/2750 ================================================>................]  Step: 40ms | Tot: 1m22s | Loss: 2.203 | Acc: 45.984% (30357/6601 2063/2750 ==================================================>..............]  Step: 40ms | Tot: 1m25s | Loss: 2.204 | Acc: 45.999% (31397/6825 2133/2750 ===================================================>.............]  Step: 40ms | Tot: 1m26s | Loss: 2.203 | Acc: 45.998% (31794/6912 2160/2750 ====================================================>............]  Step: 41ms | Tot: 1m28s | Loss: 2.202 | Acc: 45.988% (32596/7088 2215/2750 =========================================================>.......]  Step: 41ms | Tot: 1m37s | Loss: 2.202 | Acc: 46.036% (35842/7785 2433/2750 ==========================================================>......]  Step: 41ms | Tot: 1m38s | Loss: 2.202 | Acc: 46.035% (36165/7856 2455/2750 ===========================================================>.....]  Step: 42ms | Tot: 1m40s | Loss: 2.203 | Acc: 46.053% (36931/8019 2506/2750 ==============================================================>..]  Step: 40ms | Tot: 1m45s | Loss: 2.203 | Acc: 46.057% (38747/8412 2629/2750 ==============================================================>..]  Step: 41ms | Tot: 1m45s | Loss: 2.203 | Acc: 46.056% (38805/8425 2633/2750 \n",
      " [================================================================>]  Step: 17ms | Tot: 12s542ms | Loss: 2.577 | Acc: 39.183% (8614/2198 687/688 ================================================================>]  Step: 17ms | Tot: 12s421ms | Loss: 2.579 | Acc: 39.122% (8513/2176 680/688 ================================================================>]  Step: 12ms | Tot: 12s555ms | Loss: 2.577 | Acc: 39.182% (8620/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 40ms | Tot: 1m50s | Loss: 2.018 | Acc: 49.667% (43707/8800 2750/2750 0  ...........................................................]  Step: 41ms | Tot: 1s427ms | Loss: 1.929 | Acc: 51.964% (582/112 35/2750 ======>..........................................................]  Step: 40ms | Tot: 10s571ms | Loss: 1.898 | Acc: 51.961% (4373/841 263/2750 ==========>......................................................]  Step: 41ms | Tot: 17s327ms | Loss: 1.946 | Acc: 51.204% (7062/1379 431/2750 ==========>......................................................]  Step: 41ms | Tot: 18s13ms | Loss: 1.946 | Acc: 51.290% (7353/1433 448/2750 ============>....................................................]  Step: 41ms | Tot: 20s678ms | Loss: 1.942 | Acc: 51.246% (8429/1644 514/2750 =================>...............................................]  Step: 40ms | Tot: 30s84ms | Loss: 1.949 | Acc: 51.067% (12207/2390 747/2750 ==================>..............................................]  Step: 41ms | Tot: 32s189ms | Loss: 1.955 | Acc: 50.970% (13032/2556 799/2750 ===================>.............................................]  Step: 40ms | Tot: 32s431ms | Loss: 1.956 | Acc: 50.963% (13128/2576 805/2750 ===================>.............................................]  Step: 40ms | Tot: 33s976ms | Loss: 1.955 | Acc: 50.882% (13726/2697 843/2750 ======================>..........................................]  Step: 40ms | Tot: 37s697ms | Loss: 1.960 | Acc: 50.732% (15179/2992 935/2750 =======================>.........................................]  Step: 41ms | Tot: 39s547ms | Loss: 1.961 | Acc: 50.685% (15911/3139 981/2750 =============================>...................................]  Step: 41ms | Tot: 49s899ms | Loss: 1.968 | Acc: 50.543% (20023/3961 1238/2750 ==============================>..................................]  Step: 42ms | Tot: 52s183ms | Loss: 1.969 | Acc: 50.480% (20919/4144 1295/2750 ====================================================>............]  Step: 40ms | Tot: 1m29s | Loss: 2.004 | Acc: 49.940% (35509/7110 2222/2750 ====================================================>............]  Step: 40ms | Tot: 1m30s | Loss: 2.005 | Acc: 49.900% (35768/7168 2240/2750 =====================================================>...........]  Step: 40ms | Tot: 1m31s | Loss: 2.005 | Acc: 49.901% (36248/7264 2270/2750 ======================================================>..........]  Step: 41ms | Tot: 1m32s | Loss: 2.007 | Acc: 49.885% (36827/7382 2307/2750 =======================================================>.........]  Step: 40ms | Tot: 1m35s | Loss: 2.009 | Acc: 49.857% (37780/7577 2368/2750 ========================================================>........]  Step: 40ms | Tot: 1m35s | Loss: 2.008 | Acc: 49.868% (38043/7628 2384/2750 ========================================================>........]  Step: 40ms | Tot: 1m36s | Loss: 2.009 | Acc: 49.847% (38330/7689 2403/2750 =========================================================>.......]  Step: 40ms | Tot: 1m37s | Loss: 2.010 | Acc: 49.809% (38620/7753 2423/2750 =========================================================>.......]  Step: 40ms | Tot: 1m37s | Loss: 2.009 | Acc: 49.815% (38704/7769 2428/2750 =========================================================>.......]  Step: 41ms | Tot: 1m38s | Loss: 2.009 | Acc: 49.826% (38952/7817 2443/2750 =========================================================>.......]  Step: 41ms | Tot: 1m38s | Loss: 2.010 | Acc: 49.811% (39084/7846 2452/2750 ==========================================================>......]  Step: 40ms | Tot: 1m39s | Loss: 2.010 | Acc: 49.846% (39510/7926 2477/2750 ===============================================================>.]  Step: 40ms | Tot: 1m48s | Loss: 2.015 | Acc: 49.707% (42772/8604 2689/2750 ================================================================>]  Step: 40ms | Tot: 1m50s | Loss: 2.018 | Acc: 49.672% (43505/8758 2737/2750 \n",
      " [================================================================>]  Step: 12ms | Tot: 12s232ms | Loss: 2.523 | Acc: 40.805% (8977/2200 688/688 =>...........................................................]  Step: 17ms | Tot: 1s229ms | Loss: 2.528 | Acc: 41.766% (842/201 63/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      " [================================================================>]  Step: 39ms | Tot: 1m50s | Loss: 1.854 | Acc: 53.152% (46774/8800 2750/2750 0  .............................................................]  Step: 41ms | Tot: 6s116ms | Loss: 1.694 | Acc: 57.292% (2805/489 153/2750 ====>............................................................]  Step: 40ms | Tot: 7s327ms | Loss: 1.685 | Acc: 57.275% (3354/585 183/2750 ====>............................................................]  Step: 42ms | Tot: 8s251ms | Loss: 1.671 | Acc: 57.388% (3783/659 206/2750 =====>...........................................................]  Step: 41ms | Tot: 10s173ms | Loss: 1.693 | Acc: 56.767% (4614/812 254/2750 =========>.......................................................]  Step: 41ms | Tot: 15s827ms | Loss: 1.706 | Acc: 56.409% (7112/1260 394/2750 =================>...............................................]  Step: 40ms | Tot: 30s7ms | Loss: 1.748 | Acc: 55.435% (13269/2393 748/2750 ===========================>.....................................]  Step: 40ms | Tot: 47s364ms | Loss: 1.784 | Acc: 54.607% (20637/3779 1181/2750 ================================>................................]  Step: 41ms | Tot: 56s76ms | Loss: 1.797 | Acc: 54.259% (24221/4464 1395/2750 ========================================>........................]  Step: 41ms | Tot: 1m8s | Loss: 1.812 | Acc: 54.001% (29601/5481 1713/2750 ==========================================>......................]  Step: 41ms | Tot: 1m12s | Loss: 1.817 | Acc: 53.930% (31029/5753 1798/2750 ===============================================>.................]  Step: 41ms | Tot: 1m20s | Loss: 1.824 | Acc: 53.728% (34403/6403 2001/2750 =====================================================>...........]  Step: 41ms | Tot: 1m30s | Loss: 1.836 | Acc: 53.485% (38766/7248 2265/2750 ======================================================>..........]  Step: 41ms | Tot: 1m32s | Loss: 1.836 | Acc: 53.475% (39375/7363 2301/2750 =========================================================>.......]  Step: 41ms | Tot: 1m38s | Loss: 1.845 | Acc: 53.341% (41717/7820 2444/2750 ============================================================>....]  Step: 40ms | Tot: 1m42s | Loss: 1.849 | Acc: 53.268% (43347/8137 2543/2750 \n",
      " [================================================================>]  Step: 12ms | Tot: 12s21ms | Loss: 2.448 | Acc: 42.723% (9399/2200 688/688  =============>.................................................]  Step: 16ms | Tot: 3s21ms | Loss: 2.465 | Acc: 42.578% (2289/537 168/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 39ms | Tot: 1m50s | Loss: 1.704 | Acc: 56.543% (49758/8800 2750/2750 0  .............................................................]  Step: 41ms | Tot: 6s654ms | Loss: 1.504 | Acc: 61.672% (3276/531 166/2750 =====>...........................................................]  Step: 40ms | Tot: 9s518ms | Loss: 1.529 | Acc: 60.970% (4624/758 237/2750 ======>..........................................................]  Step: 40ms | Tot: 11s448ms | Loss: 1.529 | Acc: 60.669% (5533/912 285/2750 =========>.......................................................]  Step: 40ms | Tot: 16s207ms | Loss: 1.534 | Acc: 60.623% (7818/1289 403/2750 ==========>......................................................]  Step: 41ms | Tot: 17s780ms | Loss: 1.541 | Acc: 60.379% (8540/1414 442/2750 ==========>......................................................]  Step: 40ms | Tot: 17s983ms | Loss: 1.542 | Acc: 60.368% (8635/1430 447/2750 ==========>......................................................]  Step: 41ms | Tot: 18s389ms | Loss: 1.541 | Acc: 60.387% (8831/1462 457/2750 =============>...................................................]  Step: 40ms | Tot: 22s197ms | Loss: 1.554 | Acc: 60.094% (10615/1766 552/2750 ===============>.................................................]  Step: 41ms | Tot: 26s668ms | Loss: 1.566 | Acc: 59.738% (12674/2121 663/2750 ================>................................................]  Step: 41ms | Tot: 27s562ms | Loss: 1.572 | Acc: 59.653% (13076/2192 685/2750 =====================>...........................................]  Step: 40ms | Tot: 35s922ms | Loss: 1.589 | Acc: 59.084% (16865/2854 892/2750 ====================================>............................]  Step: 41ms | Tot: 1m2s | Loss: 1.643 | Acc: 57.838% (28743/4969 1553/2750 ======================================>..........................]  Step: 42ms | Tot: 1m5s | Loss: 1.649 | Acc: 57.737% (29968/5190 1622/2750 =======================================>.........................]  Step: 41ms | Tot: 1m6s | Loss: 1.651 | Acc: 57.706% (30524/5289 1653/2750 ==========================================>......................]  Step: 41ms | Tot: 1m12s | Loss: 1.661 | Acc: 57.445% (33272/5792 1810/2750 ================================================>................]  Step: 41ms | Tot: 1m22s | Loss: 1.676 | Acc: 57.117% (37341/6537 2043/2750 ==================================================>..............]  Step: 40ms | Tot: 1m25s | Loss: 1.682 | Acc: 56.957% (38822/6816 2130/2750 =====================================================>...........]  Step: 40ms | Tot: 1m31s | Loss: 1.686 | Acc: 56.876% (41278/7257 2268/2750 ===============================================================>.]  Step: 40ms | Tot: 1m47s | Loss: 1.701 | Acc: 56.616% (48355/8540 2669/2750 ===============================================================>.]  Step: 41ms | Tot: 1m47s | Loss: 1.701 | Acc: 56.629% (48583/8579 2681/2750 ===============================================================>.]  Step: 41ms | Tot: 1m48s | Loss: 1.701 | Acc: 56.619% (48846/8627 2696/2750 \n",
      " [================================================================>]  Step: 12ms | Tot: 12s24ms | Loss: 2.505 | Acc: 42.095% (9261/2200 688/688  ......................................................]  Step: 58ms | Tot: 82ms | Loss: 2.792 | Acc: 38.542% (37/9 3/688 >................................................................]  Step: 19ms | Tot: 122ms | Loss: 2.819 | Acc: 38.750% (62/16 5/688 \n",
      "\n",
      "Epoch: 9\n",
      " [================================================================>]  Step: 40ms | Tot: 1m50s | Loss: 1.554 | Acc: 59.215% (52109/8800 2750/2750 0  ======>........................................................]  Step: 41ms | Tot: 15s112ms | Loss: 1.306 | Acc: 65.136% (7858/1206 377/2750 =========>.......................................................]  Step: 41ms | Tot: 15s639ms | Loss: 1.312 | Acc: 65.000% (8112/1248 390/2750 =========>.......................................................]  Step: 41ms | Tot: 16s931ms | Loss: 1.316 | Acc: 64.981% (8775/1350 422/2750 ===========>.....................................................]  Step: 41ms | Tot: 18s749ms | Loss: 1.323 | Acc: 64.795% (9683/1494 467/2750 =============>...................................................]  Step: 41ms | Tot: 22s437ms | Loss: 1.355 | Acc: 64.186% (11461/1785 558/2750 ======================>..........................................]  Step: 41ms | Tot: 38s705ms | Loss: 1.417 | Acc: 62.351% (19194/3078 962/2750 ========================>........................................]  Step: 41ms | Tot: 41s857ms | Loss: 1.426 | Acc: 62.148% (20683/3328 1040/2750 ===========================>.....................................]  Step: 41ms | Tot: 46s937ms | Loss: 1.435 | Acc: 61.889% (23092/3731 1166/2750 ==============================>..................................]  Step: 41ms | Tot: 51s701ms | Loss: 1.447 | Acc: 61.707% (25374/4112 1285/2750 ======================================>..........................]  Step: 41ms | Tot: 1m5s | Loss: 1.488 | Acc: 60.678% (31863/5251 1641/2750 =======================================>.........................]  Step: 41ms | Tot: 1m7s | Loss: 1.492 | Acc: 60.602% (32696/5395 1686/2750 ========================================>........................]  Step: 41ms | Tot: 1m8s | Loss: 1.492 | Acc: 60.585% (32997/5446 1702/2750 =========================================>.......................]  Step: 40ms | Tot: 1m10s | Loss: 1.495 | Acc: 60.511% (33983/5616 1755/2750 ===========================================>.....................]  Step: 41ms | Tot: 1m14s | Loss: 1.503 | Acc: 60.271% (35507/5891 1841/2750 ===============================================>.................]  Step: 40ms | Tot: 1m20s | Loss: 1.515 | Acc: 60.014% (38217/6368 1990/2750 ===============================================>.................]  Step: 40ms | Tot: 1m20s | Loss: 1.515 | Acc: 60.011% (38522/6419 2006/2750 ====================================================>............]  Step: 40ms | Tot: 1m29s | Loss: 1.528 | Acc: 59.690% (42289/7084 2214/2750 ======================================================>..........]  Step: 41ms | Tot: 1m33s | Loss: 1.533 | Acc: 59.591% (44221/7420 2319/2750 =======================================================>.........]  Step: 41ms | Tot: 1m34s | Loss: 1.536 | Acc: 59.504% (44880/7542 2357/2750 =======================================================>.........]  Step: 41ms | Tot: 1m35s | Loss: 1.537 | Acc: 59.495% (45045/7571 2366/2750 =========================================================>.......]  Step: 41ms | Tot: 1m38s | Loss: 1.541 | Acc: 59.406% (46384/7808 2440/2750 ==========================================================>......]  Step: 40ms | Tot: 1m39s | Loss: 1.542 | Acc: 59.402% (47065/7923 2476/2750 \n",
      " [================================================================>]  Step: 13ms | Tot: 12s317ms | Loss: 2.590 | Acc: 40.795% (8975/2200 688/688 \n",
      "\n",
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 40ms | Tot: 1m50s | Loss: 1.412 | Acc: 62.358% (54875/8800 2750/2750 0  ............................................................]  Step: 40ms | Tot: 2s223ms | Loss: 1.101 | Acc: 70.982% (1272/179 56/2750 =========>.......................................................]  Step: 41ms | Tot: 16s329ms | Loss: 1.149 | Acc: 69.171% (9031/1305 408/2750 =============>...................................................]  Step: 40ms | Tot: 22s271ms | Loss: 1.184 | Acc: 68.194% (12133/1779 556/2750 ==============>..................................................]  Step: 41ms | Tot: 25s89ms | Loss: 1.192 | Acc: 67.831% (13588/2003 626/2750 ===============>.................................................]  Step: 41ms | Tot: 25s774ms | Loss: 1.197 | Acc: 67.695% (13929/2057 643/2750 =================>...............................................]  Step: 40ms | Tot: 30s475ms | Loss: 1.224 | Acc: 66.974% (16288/2432 760/2750 ===========================>.....................................]  Step: 41ms | Tot: 47s269ms | Loss: 1.286 | Acc: 65.367% (24599/3763 1176/2750 ==============================>..................................]  Step: 41ms | Tot: 51s627ms | Loss: 1.299 | Acc: 65.016% (26672/4102 1282/2750 ===========================================>.....................]  Step: 42ms | Tot: 1m13s | Loss: 1.353 | Acc: 63.794% (37276/5843 1826/2750 ===========================================>.....................]  Step: 40ms | Tot: 1m13s | Loss: 1.353 | Acc: 63.798% (37442/5868 1834/2750 ============================================>....................]  Step: 40ms | Tot: 1m15s | Loss: 1.355 | Acc: 63.715% (38066/5974 1867/2750 ==============================================>..................]  Step: 41ms | Tot: 1m19s | Loss: 1.363 | Acc: 63.498% (40009/6300 1969/2750 ==============================================>..................]  Step: 40ms | Tot: 1m19s | Loss: 1.363 | Acc: 63.481% (40201/6332 1979/2750 ===============================================>.................]  Step: 41ms | Tot: 1m20s | Loss: 1.366 | Acc: 63.429% (40838/6438 2012/2750 ===============================================>.................]  Step: 41ms | Tot: 1m21s | Loss: 1.366 | Acc: 63.428% (41162/6489 2028/2750 ================================================>................]  Step: 41ms | Tot: 1m22s | Loss: 1.368 | Acc: 63.354% (41783/6595 2061/2750 =================================================>...............]  Step: 41ms | Tot: 1m23s | Loss: 1.370 | Acc: 63.324% (42108/6649 2078/2750 ==================================================>..............]  Step: 41ms | Tot: 1m26s | Loss: 1.376 | Acc: 63.162% (43233/6844 2139/2750 ===================================================>.............]  Step: 40ms | Tot: 1m27s | Loss: 1.380 | Acc: 63.082% (43804/6944 2170/2750 ===================================================>.............]  Step: 40ms | Tot: 1m27s | Loss: 1.380 | Acc: 63.070% (44119/6995 2186/2750 ===================================================>.............]  Step: 41ms | Tot: 1m28s | Loss: 1.380 | Acc: 63.085% (44210/7008 2190/2750 ====================================================>............]  Step: 40ms | Tot: 1m29s | Loss: 1.382 | Acc: 63.044% (45049/7145 2233/2750 =====================================================>...........]  Step: 41ms | Tot: 1m30s | Loss: 1.383 | Acc: 63.011% (45247/7180 2244/2750 ======================================================>..........]  Step: 40ms | Tot: 1m32s | Loss: 1.386 | Acc: 62.970% (46185/7334 2292/2750 =========================================================>.......]  Step: 40ms | Tot: 1m38s | Loss: 1.395 | Acc: 62.769% (48990/7804 2439/2750 ==========================================================>......]  Step: 40ms | Tot: 1m38s | Loss: 1.396 | Acc: 62.744% (49372/7868 2459/2750 ==========================================================>......]  Step: 40ms | Tot: 1m39s | Loss: 1.396 | Acc: 62.739% (49408/7875 2461/2750 ===========================================================>.....]  Step: 40ms | Tot: 1m40s | Loss: 1.397 | Acc: 62.708% (50146/7996 2499/2750 ===========================================================>.....]  Step: 41ms | Tot: 1m41s | Loss: 1.398 | Acc: 62.688% (50592/8070 2522/2750 ============================================================>....]  Step: 41ms | Tot: 1m43s | Loss: 1.401 | Acc: 62.624% (51642/8246 2577/2750 =============================================================>...]  Step: 41ms | Tot: 1m44s | Loss: 1.403 | Acc: 62.585% (52051/8316 2599/2750 =============================================================>...]  Step: 40ms | Tot: 1m44s | Loss: 1.403 | Acc: 62.577% (52184/8339 2606/2750 ==============================================================>..]  Step: 41ms | Tot: 1m46s | Loss: 1.405 | Acc: 62.518% (52835/8451 2641/2750 ==============================================================>..]  Step: 41ms | Tot: 1m47s | Loss: 1.406 | Acc: 62.479% (53262/8524 2664/2750 ===============================================================>.]  Step: 41ms | Tot: 1m47s | Loss: 1.407 | Acc: 62.472% (53376/8544 2670/2750 ===============================================================>.]  Step: 41ms | Tot: 1m48s | Loss: 1.408 | Acc: 62.473% (53937/8633 2698/2750 ================================================================>]  Step: 41ms | Tot: 1m50s | Loss: 1.411 | Acc: 62.402% (54554/8742 2732/2750 \n",
      " [================================================================>]  Step: 12ms | Tot: 12s192ms | Loss: 2.652 | Acc: 40.823% (8981/2200 688/688 \n",
      "\n",
      "Epoch: 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 41ms | Tot: 1m49s | Loss: 1.263 | Acc: 65.802% (57906/8800 2750/2750 0  =========>......................................................]  Step: 40ms | Tot: 17s20ms | Loss: 0.970 | Acc: 73.608% (10364/1408 440/2750 =================>...............................................]  Step: 40ms | Tot: 28s633ms | Loss: 1.033 | Acc: 71.631% (16710/2332 729/2750 ==================>..............................................]  Step: 40ms | Tot: 30s840ms | Loss: 1.045 | Acc: 71.361% (17903/2508 784/2750 ===========================>.....................................]  Step: 40ms | Tot: 46s225ms | Loss: 1.118 | Acc: 69.548% (25972/3734 1167/2750 =============================>...................................]  Step: 41ms | Tot: 48s841ms | Loss: 1.129 | Acc: 69.257% (27304/3942 1232/2750 =============================>...................................]  Step: 41ms | Tot: 49s245ms | Loss: 1.131 | Acc: 69.188% (27498/3974 1242/2750 ===============================>.................................]  Step: 41ms | Tot: 52s111ms | Loss: 1.141 | Acc: 68.955% (28972/4201 1313/2750 ===============================>.................................]  Step: 41ms | Tot: 52s481ms | Loss: 1.141 | Acc: 68.939% (29164/4230 1322/2750 ================================>................................]  Step: 41ms | Tot: 55s179ms | Loss: 1.150 | Acc: 68.750% (30558/4444 1389/2750 =================================>...............................]  Step: 40ms | Tot: 56s604ms | Loss: 1.154 | Acc: 68.664% (31289/4556 1424/2750 ===================================>.............................]  Step: 41ms | Tot: 59s420ms | Loss: 1.162 | Acc: 68.468% (32733/4780 1494/2750 ===================================>.............................]  Step: 41ms | Tot: 59s947ms | Loss: 1.163 | Acc: 68.462% (33015/4822 1507/2750 ====================================>............................]  Step: 41ms | Tot: 1m995ms | Loss: 1.167 | Acc: 68.359% (33534/4905 1533/2750 ====================================>............................]  Step: 41ms | Tot: 1m1s | Loss: 1.169 | Acc: 68.309% (33794/4947 1546/2750 =========================================>.......................]  Step: 41ms | Tot: 1m9s | Loss: 1.184 | Acc: 67.948% (37877/5574 1742/2750 =========================================>.......................]  Step: 41ms | Tot: 1m10s | Loss: 1.188 | Acc: 67.842% (38556/5683 1776/2750 ==========================================>......................]  Step: 41ms | Tot: 1m12s | Loss: 1.191 | Acc: 67.785% (39413/5814 1817/2750 ============================================>....................]  Step: 40ms | Tot: 1m15s | Loss: 1.198 | Acc: 67.597% (41056/6073 1898/2750 =============================================>...................]  Step: 41ms | Tot: 1m17s | Loss: 1.200 | Acc: 67.545% (41975/6214 1942/2750 ===============================================>.................]  Step: 41ms | Tot: 1m19s | Loss: 1.202 | Acc: 67.489% (43150/6393 1998/2750 =======================================================>.........]  Step: 40ms | Tot: 1m34s | Loss: 1.237 | Acc: 66.545% (50297/7558 2362/2750 =============================================================>...]  Step: 41ms | Tot: 1m43s | Loss: 1.251 | Acc: 66.111% (54666/8268 2584/2750 ==============================================================>..]  Step: 41ms | Tot: 1m45s | Loss: 1.256 | Acc: 66.002% (55653/8432 2635/2750 \n",
      " [================================================================>]  Step: 12ms | Tot: 12s111ms | Loss: 2.925 | Acc: 38.836% (8544/2200 688/688 ==>..........................................................]  Step: 17ms | Tot: 1s386ms | Loss: 3.027 | Acc: 38.375% (921/240 75/688 =======>.........................................................]  Step: 17ms | Tot: 1s440ms | Loss: 3.021 | Acc: 38.061% (950/249 78/688 ==========================>......................................]  Step: 17ms | Tot: 4s909ms | Loss: 2.946 | Acc: 38.662% (3427/886 277/688 \n",
      "\n",
      "Epoch: 12\n",
      " [================================================================>]  Step: 40ms | Tot: 1m50s | Loss: 1.140 | Acc: 68.667% (60427/8800 2750/2750 0  >............................................................]  Step: 42ms | Tot: 7s159ms | Loss: 0.838 | Acc: 77.288% (4526/585 183/2750 ==========>......................................................]  Step: 40ms | Tot: 18s190ms | Loss: 0.857 | Acc: 76.405% (11149/1459 456/2750 ==============>..................................................]  Step: 40ms | Tot: 25s391ms | Loss: 0.892 | Acc: 75.369% (15315/2032 635/2750 ===============>.................................................]  Step: 40ms | Tot: 25s995ms | Loss: 0.894 | Acc: 75.317% (15666/2080 650/2750 ===================>.............................................]  Step: 41ms | Tot: 33s439ms | Loss: 0.921 | Acc: 74.540% (19917/2672 835/2750 ======================>..........................................]  Step: 41ms | Tot: 38s946ms | Loss: 0.957 | Acc: 73.524% (22869/3110 972/2750 ==========================>......................................]  Step: 40ms | Tot: 44s476ms | Loss: 0.974 | Acc: 73.026% (25939/3552 1110/2750 ============================>....................................]  Step: 41ms | Tot: 49s165ms | Loss: 0.991 | Acc: 72.627% (28493/3923 1226/2750 =============================>...................................]  Step: 40ms | Tot: 50s169ms | Loss: 0.992 | Acc: 72.592% (29060/4003 1251/2750 ===================================>.............................]  Step: 40ms | Tot: 1m472ms | Loss: 1.025 | Acc: 71.589% (34546/4825 1508/2750 ===================================>.............................]  Step: 40ms | Tot: 1m552ms | Loss: 1.026 | Acc: 71.587% (34591/4832 1510/2750 ====================================>............................]  Step: 40ms | Tot: 1m2s | Loss: 1.031 | Acc: 71.427% (35679/4995 1561/2750 =====================================>...........................]  Step: 41ms | Tot: 1m3s | Loss: 1.033 | Acc: 71.351% (36052/5052 1579/2750 ======================================>..........................]  Step: 40ms | Tot: 1m5s | Loss: 1.038 | Acc: 71.222% (37058/5203 1626/2750 =========================================>.......................]  Step: 41ms | Tot: 1m9s | Loss: 1.048 | Acc: 70.972% (39472/5561 1738/2750 =========================================>.......................]  Step: 40ms | Tot: 1m9s | Loss: 1.049 | Acc: 70.958% (39555/5574 1742/2750 ================================================================>]  Step: 41ms | Tot: 1m48s | Loss: 1.137 | Acc: 68.748% (59618/8672 2710/2750 ================================================================>]  Step: 40ms | Tot: 1m49s | Loss: 1.137 | Acc: 68.741% (59942/8720 2725/2750 \n",
      " [================================================================>]  Step: 12ms | Tot: 12s141ms | Loss: 2.813 | Acc: 40.082% (8818/2200 688/688 ================================>...............................]  Step: 17ms | Tot: 6s427ms | Loss: 2.807 | Acc: 40.111% (4608/1148 359/688 \n",
      "\n",
      "Epoch: 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 41ms | Tot: 1m50s | Loss: 1.020 | Acc: 71.374% (62809/8800 2750/2750 0  =>...........................................................]  Step: 41ms | Tot: 9s297ms | Loss: 0.677 | Acc: 81.397% (6095/748 234/2750 ======>..........................................................]  Step: 39ms | Tot: 11s110ms | Loss: 0.680 | Acc: 81.149% (7245/892 279/2750 =======>.........................................................]  Step: 41ms | Tot: 12s158ms | Loss: 0.682 | Acc: 81.045% (7910/976 305/2750 =========>.......................................................]  Step: 41ms | Tot: 16s329ms | Loss: 0.694 | Acc: 80.723% (10565/1308 409/2750 ==========>......................................................]  Step: 41ms | Tot: 17s419ms | Loss: 0.705 | Acc: 80.297% (11203/1395 436/2750 ===========>.....................................................]  Step: 42ms | Tot: 19s506ms | Loss: 0.718 | Acc: 79.921% (12506/1564 489/2750 ================>................................................]  Step: 40ms | Tot: 27s924ms | Loss: 0.747 | Acc: 78.904% (17624/2233 698/2750 ====================>............................................]  Step: 41ms | Tot: 35s447ms | Loss: 0.785 | Acc: 77.825% (22065/2835 886/2750 ==============================>..................................]  Step: 40ms | Tot: 52s314ms | Loss: 0.851 | Acc: 76.012% (31694/4169 1303/2750 ===============================>.................................]  Step: 40ms | Tot: 52s797ms | Loss: 0.852 | Acc: 75.970% (31968/4208 1315/2750 ===================================>.............................]  Step: 41ms | Tot: 1m124ms | Loss: 0.883 | Acc: 75.075% (35940/4787 1496/2750 ======================================>..........................]  Step: 40ms | Tot: 1m6s | Loss: 0.908 | Acc: 74.356% (39260/5280 1650/2750 ==========================================>......................]  Step: 41ms | Tot: 1m11s | Loss: 0.923 | Acc: 73.912% (42124/5699 1781/2750 ==============================================>..................]  Step: 40ms | Tot: 1m18s | Loss: 0.941 | Acc: 73.401% (45826/6243 1951/2750 ===============================================>.................]  Step: 41ms | Tot: 1m20s | Loss: 0.947 | Acc: 73.240% (46803/6390 1997/2750 =======================================================>.........]  Step: 41ms | Tot: 1m34s | Loss: 0.986 | Acc: 72.283% (54334/7516 2349/2750 ===========================================================>.....]  Step: 41ms | Tot: 1m41s | Loss: 1.002 | Acc: 71.870% (58301/8112 2535/2750 \n",
      " [================================================================>]  Step: 12ms | Tot: 12s288ms | Loss: 2.995 | Acc: 39.791% (8754/2200 688/688 \n"
     ]
    }
   ],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "net = PreActResNet18(in_dim=in_dim,num_classes=len(classes))\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "early_stopping = EarlyStopping(tolerance=5, min_delta=1)\n",
    "\n",
    "epoch = 0\n",
    "while(not(early_stopping.early_stop)):\n",
    "    train_loss = train(epoch) / len(train_dataloader)\n",
    "    validation_loss = test(epoch,'PreActResNet') / len(test_dataloader)\n",
    "    scheduler.step()\n",
    "    early_stopping(train_loss,validation_loss)\n",
    "    epoch+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8hqQsNKvtI4f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " [================================================================>]  Step: 274ms | Tot: 12m34s | Loss: 5.087 | Acc: 2.772% (2439/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 49ms | Tot: 44s348ms | Loss: 4.811 | Acc: 7.018% (1544/2200 688/688 =======================>.......................................]  Step: 67ms | Tot: 17s588ms | Loss: 4.852 | Acc: 7.200% (629/873 273/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [================================================================>]  Step: 275ms | Tot: 12m34s | Loss: 4.082 | Acc: 12.791% (11256/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 37ms | Tot: 44s384ms | Loss: 5.005 | Acc: 18.677% (4109/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      " [================================================================>]  Step: 275ms | Tot: 12m34s | Loss: 3.358 | Acc: 23.956% (21081/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 37ms | Tot: 44s365ms | Loss: 5.714 | Acc: 25.664% (5646/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      " [================================================================>]  Step: 281ms | Tot: 12m34s | Loss: 2.910 | Acc: 31.982% (28144/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 37ms | Tot: 44s636ms | Loss: 5.860 | Acc: 30.632% (6739/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      " [================================================================>]  Step: 274ms | Tot: 12m34s | Loss: 2.580 | Acc: 38.341% (33740/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 37ms | Tot: 44s433ms | Loss: 2.662 | Acc: 37.905% (8339/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      " [================================================================>]  Step: 274ms | Tot: 12m35s | Loss: 2.316 | Acc: 43.657% (38418/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 37ms | Tot: 44s350ms | Loss: 2.791 | Acc: 38.914% (8561/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      " [================================================================>]  Step: 273ms | Tot: 12m34s | Loss: 2.099 | Acc: 48.284% (42490/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 38ms | Tot: 44s374ms | Loss: 2.517 | Acc: 40.591% (8930/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      " [================================================================>]  Step: 276ms | Tot: 12m35s | Loss: 1.917 | Acc: 51.909% (45680/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 37ms | Tot: 44s455ms | Loss: 2.478 | Acc: 41.336% (9094/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 8\n",
      " [================================================================>]  Step: 274ms | Tot: 12m34s | Loss: 1.752 | Acc: 55.349% (48707/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 37ms | Tot: 44s463ms | Loss: 2.702 | Acc: 43.736% (9622/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      " [================================================================>]  Step: 274ms | Tot: 12m35s | Loss: 1.616 | Acc: 58.530% (51506/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 37ms | Tot: 44s517ms | Loss: 2.519 | Acc: 44.405% (9769/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 10\n",
      " [================================================================>]  Step: 274ms | Tot: 12m34s | Loss: 1.486 | Acc: 61.197% (53853/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 37ms | Tot: 44s112ms | Loss: 2.310 | Acc: 46.300% (10186/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 11\n",
      " [================================================================>]  Step: 274ms | Tot: 12m33s | Loss: 1.364 | Acc: 63.837% (56177/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 37ms | Tot: 44s224ms | Loss: 2.367 | Acc: 46.464% (10222/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 12\n",
      " [================================================================>]  Step: 274ms | Tot: 12m34s | Loss: 1.252 | Acc: 66.509% (58528/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 37ms | Tot: 44s152ms | Loss: 2.630 | Acc: 43.405% (9549/2200 688/688 \n",
      "\n",
      "Epoch: 13\n",
      " [================================================================>]  Step: 273ms | Tot: 12m33s | Loss: 1.165 | Acc: 68.368% (60164/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 44ms | Tot: 44s72ms | Loss: 2.204 | Acc: 48.173% (10598/2200 688/688  =============================================>..................]  Step: 66ms | Tot: 31s268ms | Loss: 2.218 | Acc: 47.925% (7484/1561 488/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 14\n",
      " [================================================================>]  Step: 273ms | Tot: 12m32s | Loss: 1.068 | Acc: 70.845% (62344/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 36ms | Tot: 43s915ms | Loss: 2.591 | Acc: 46.436% (10216/2200 688/688 \n",
      "\n",
      "Epoch: 15\n",
      " [================================================================>]  Step: 275ms | Tot: 12m33s | Loss: 1.015 | Acc: 72.009% (63368/8800 2750/2750 0 \n",
      " [================================================================>]  Step: 37ms | Tot: 44s435ms | Loss: 2.555 | Acc: 44.650% (9823/2200 688/688 ==================================================>..............]  Step: 67ms | Tot: 34s403ms | Loss: 2.570 | Acc: 44.436% (7579/1705 533/688 ===========================================================>.....]  Step: 67ms | Tot: 40s962ms | Loss: 2.558 | Acc: 44.706% (9070/2028 634/688 \n"
     ]
    }
   ],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "net = DPN92(in_dim=in_dim,num_classes=len(classes))\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "early_stopping = EarlyStopping(tolerance=5, min_delta=1)\n",
    "\n",
    "epoch = 0\n",
    "while(not(early_stopping.early_stop)):\n",
    "    train_loss = train(epoch) / len(train_dataloader)\n",
    "    validation_loss = test(epoch,'DPN') / len(test_dataloader)\n",
    "    scheduler.step()\n",
    "    early_stopping(train_loss,validation_loss)\n",
    "    epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZeKELZ7tLCp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " [================================================================>]  Step: 77ms | Tot: 3m21s | Loss: 4.379 | Acc: 8.510% (7489/8800 2750/2750 0 =========================================================>.......]  Step: 67ms | Tot: 3m345ms | Loss: 4.433 | Acc: 7.864% (6173/7849 2453/2750 ===========================================================>.....]  Step: 67ms | Tot: 3m3s | Loss: 4.424 | Acc: 7.993% (6405/8012 2504/2750 \n",
      " [================================================================>]  Step: 29ms | Tot: 17s441ms | Loss: 3.884 | Acc: 14.214% (3127/2200 688/688 ================>.............................................]  Step: 25ms | Tot: 5s336ms | Loss: 3.901 | Acc: 13.863% (936/675 211/688 ==========================>......................................]  Step: 25ms | Tot: 7s108ms | Loss: 3.890 | Acc: 14.074% (1261/896 280/688 ==========================>......................................]  Step: 26ms | Tot: 7s134ms | Loss: 3.889 | Acc: 14.057% (1264/899 281/688 ==========================================>......................]  Step: 27ms | Tot: 11s322ms | Loss: 3.891 | Acc: 13.954% (1996/1430 447/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [================================================================>]  Step: 67ms | Tot: 3m15s | Loss: 3.635 | Acc: 18.137% (15961/8800 2750/2750 0 .............................................................]  Step: 69ms | Tot: 7s186ms | Loss: 3.788 | Acc: 15.254% (493/323 101/2750 ========>........................................................]  Step: 65ms | Tot: 26s193ms | Loss: 3.810 | Acc: 15.239% (1775/1164 364/2750 ========>........................................................]  Step: 68ms | Tot: 26s329ms | Loss: 3.810 | Acc: 15.224% (1783/1171 366/2750 ==================================================>..............]  Step: 66ms | Tot: 2m33s | Loss: 3.673 | Acc: 17.556% (12090/6886 2152/2750 =======================================================>.........]  Step: 66ms | Tot: 2m48s | Loss: 3.660 | Acc: 17.753% (13367/7529 2353/2750 ============================================================>....]  Step: 66ms | Tot: 3m3s | Loss: 3.648 | Acc: 17.936% (14802/8252 2579/2750 \n",
      " [================================================================>]  Step: 22ms | Tot: 17s193ms | Loss: 3.468 | Acc: 21.214% (4667/2200 688/688 ===============================>................................]  Step: 27ms | Tot: 8s476ms | Loss: 3.472 | Acc: 20.993% (2284/1088 340/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      " [================================================================>]  Step: 64ms | Tot: 3m11s | Loss: 3.282 | Acc: 24.390% (21463/8800 2750/2750 0 =======>........................................................]  Step: 67ms | Tot: 26s442ms | Loss: 3.326 | Acc: 23.416% (2735/1168 365/2750 ============>....................................................]  Step: 66ms | Tot: 38s863ms | Loss: 3.347 | Acc: 23.299% (4026/1728 540/2750 ======================>..........................................]  Step: 67ms | Tot: 1m6s | Loss: 3.347 | Acc: 23.239% (7087/3049 953/2750 ==============================================>..................]  Step: 65ms | Tot: 2m18s | Loss: 3.305 | Acc: 23.933% (15095/6307 1971/2750 ====================================================>............]  Step: 65ms | Tot: 2m34s | Loss: 3.300 | Acc: 24.051% (16955/7049 2203/2750 ===========================================================>.....]  Step: 70ms | Tot: 2m55s | Loss: 3.287 | Acc: 24.312% (19465/8006 2502/2750 ==============================================================>..]  Step: 70ms | Tot: 3m3s | Loss: 3.283 | Acc: 24.375% (20491/8406 2627/2750 \n",
      " [================================================================>]  Step: 23ms | Tot: 17s76ms | Loss: 3.191 | Acc: 25.923% (5703/2200 688/688  \n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      " [================================================================>]  Step: 68ms | Tot: 3m9s | Loss: 3.050 | Acc: 28.590% (25159/8800 2750/2750 50 ............................................................]  Step: 67ms | Tot: 3s58ms | Loss: 3.007 | Acc: 29.869% (411/137 43/2750 =>...............................................................]  Step: 65ms | Tot: 5s111ms | Loss: 3.019 | Acc: 29.238% (683/233 73/2750 ============>....................................................]  Step: 67ms | Tot: 37s106ms | Loss: 3.028 | Acc: 28.892% (4937/1708 534/2750 =============>...................................................]  Step: 68ms | Tot: 39s405ms | Loss: 3.036 | Acc: 28.797% (5225/1814 567/2750 ====================>............................................]  Step: 67ms | Tot: 1m1s | Loss: 3.045 | Acc: 28.663% (8145/2841 888/2750 =====================================>...........................]  Step: 68ms | Tot: 1m49s | Loss: 3.058 | Acc: 28.313% (14451/5104 1595/2750 ==============================================>..................]  Step: 64ms | Tot: 2m14s | Loss: 3.055 | Acc: 28.440% (17801/6259 1956/2750 ===================================================>.............]  Step: 65ms | Tot: 2m30s | Loss: 3.051 | Acc: 28.484% (19898/6985 2183/2750 \n",
      " [================================================================>]  Step: 24ms | Tot: 16s927ms | Loss: 3.030 | Acc: 29.150% (6413/2200 688/688 ========================>......................................]  Step: 24ms | Tot: 6s853ms | Loss: 3.036 | Acc: 28.503% (2563/899 281/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      " [================================================================>]  Step: 70ms | Tot: 3m11s | Loss: 2.874 | Acc: 31.923% (28092/8800 2750/2750 0 =>.............................................................]  Step: 66ms | Tot: 11s545ms | Loss: 2.834 | Acc: 32.365% (1626/502 157/2750 =====>...........................................................]  Step: 67ms | Tot: 16s998ms | Loss: 2.832 | Acc: 32.587% (2461/755 236/2750 =========>.......................................................]  Step: 72ms | Tot: 29s431ms | Loss: 2.834 | Acc: 32.503% (4358/1340 419/2750 =============>...................................................]  Step: 72ms | Tot: 41s395ms | Loss: 2.840 | Acc: 32.235% (6117/1897 593/2750 ===========================>.....................................]  Step: 67ms | Tot: 1m19s | Loss: 2.858 | Acc: 31.852% (11691/3670 1147/2750 ============================>....................................]  Step: 67ms | Tot: 1m23s | Loss: 2.859 | Acc: 31.832% (12305/3865 1208/2750 =============================>...................................]  Step: 67ms | Tot: 1m26s | Loss: 2.859 | Acc: 31.848% (12627/3964 1239/2750 ========================================>........................]  Step: 70ms | Tot: 1m58s | Loss: 2.860 | Acc: 31.908% (17501/5484 1714/2750 =========================================>.......................]  Step: 66ms | Tot: 2m870ms | Loss: 2.861 | Acc: 31.901% (17783/5574 1742/2750 ==============================================>..................]  Step: 66ms | Tot: 2m17s | Loss: 2.867 | Acc: 31.831% (20097/6313 1973/2750 \n",
      " [================================================================>]  Step: 24ms | Tot: 17s166ms | Loss: 2.970 | Acc: 29.877% (6573/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      " [================================================================>]  Step: 65ms | Tot: 3m18s | Loss: 2.739 | Acc: 34.666% (30506/8800 2750/2750 0 =====================>..........................................]  Step: 66ms | Tot: 1m11s | Loss: 2.706 | Acc: 34.962% (10774/3081 963/2750 ===========================>.....................................]  Step: 66ms | Tot: 1m24s | Loss: 2.716 | Acc: 34.787% (12757/3667 1146/2750 =================================================>...............]  Step: 66ms | Tot: 2m30s | Loss: 2.742 | Acc: 34.504% (23087/6691 2091/2750 \n",
      " [================================================================>]  Step: 22ms | Tot: 17s688ms | Loss: 2.812 | Acc: 33.759% (7427/2200 688/688 ==========================================>......................]  Step: 25ms | Tot: 11s694ms | Loss: 2.790 | Acc: 34.237% (4952/1446 452/688 ========================================================>........]  Step: 25ms | Tot: 15s441ms | Loss: 2.806 | Acc: 33.931% (6493/1913 598/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 71ms | Tot: 3m13s | Loss: 2.641 | Acc: 36.685% (32283/8800 2750/2750 0 =====>.........................................................]  Step: 67ms | Tot: 21s903ms | Loss: 2.570 | Acc: 37.874% (3745/988 309/2750 ============>....................................................]  Step: 67ms | Tot: 37s137ms | Loss: 2.588 | Acc: 37.524% (6280/1673 523/2750 ============================>....................................]  Step: 66ms | Tot: 1m27s | Loss: 2.614 | Acc: 37.152% (14540/3913 1223/2750 =========================================================>.......]  Step: 65ms | Tot: 2m51s | Loss: 2.635 | Acc: 36.768% (28591/7776 2430/2750 \n",
      " [================================================================>]  Step: 22ms | Tot: 17s558ms | Loss: 2.716 | Acc: 35.709% (7856/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      " [================================================================>]  Step: 66ms | Tot: 3m14s | Loss: 2.555 | Acc: 38.326% (33727/8800 2750/2750 0 ............................................................]  Step: 69ms | Tot: 4s426ms | Loss: 2.493 | Acc: 39.795% (815/204 64/2750 =====>...........................................................]  Step: 69ms | Tot: 17s440ms | Loss: 2.491 | Acc: 39.776% (3233/812 254/2750 ======>..........................................................]  Step: 67ms | Tot: 17s846ms | Loss: 2.486 | Acc: 39.940% (3323/832 260/2750 ==============>..................................................]  Step: 68ms | Tot: 44s530ms | Loss: 2.495 | Acc: 39.664% (8047/2028 634/2750 =========================>.......................................]  Step: 67ms | Tot: 1m18s | Loss: 2.519 | Acc: 39.127% (13710/3504 1095/2750 ===========================>.....................................]  Step: 66ms | Tot: 1m24s | Loss: 2.523 | Acc: 39.020% (14709/3769 1178/2750 ============================================>....................]  Step: 73ms | Tot: 2m13s | Loss: 2.541 | Acc: 38.633% (23093/5977 1868/2750 \n",
      " [================================================================>]  Step: 22ms | Tot: 17s23ms | Loss: 2.699 | Acc: 35.986% (7917/2200 688/688  \n",
      "Saving..\n",
      "\n",
      "Epoch: 8\n",
      " [================================================================>]  Step: 75ms | Tot: 3m12s | Loss: 2.491 | Acc: 39.786% (35012/8800 2750/2750 0 ...........................................................]  Step: 68ms | Tot: 2s211ms | Loss: 2.322 | Acc: 44.434% (455/102 32/2750 =========>.......................................................]  Step: 71ms | Tot: 27s760ms | Loss: 2.401 | Acc: 41.503% (5485/1321 413/2750 ==========================================>......................]  Step: 67ms | Tot: 2m6s | Loss: 2.467 | Acc: 40.217% (23139/5753 1798/2750 ===========================================>.....................]  Step: 66ms | Tot: 2m8s | Loss: 2.469 | Acc: 40.173% (23551/5862 1832/2750 =============================================>...................]  Step: 71ms | Tot: 2m17s | Loss: 2.471 | Acc: 40.123% (24998/6230 1947/2750 ===================================================>.............]  Step: 66ms | Tot: 2m32s | Loss: 2.478 | Acc: 40.029% (27822/6950 2172/2750 ====================================================>............]  Step: 66ms | Tot: 2m36s | Loss: 2.481 | Acc: 39.966% (28584/7152 2235/2750 ===========================================================>.....]  Step: 69ms | Tot: 2m57s | Loss: 2.486 | Acc: 39.874% (32371/8118 2537/2750 ==============================================================>..]  Step: 70ms | Tot: 3m3s | Loss: 2.488 | Acc: 39.842% (33518/8412 2629/2750 ==============================================================>..]  Step: 65ms | Tot: 3m5s | Loss: 2.487 | Acc: 39.854% (33809/8483 2651/2750 \n",
      " [================================================================>]  Step: 22ms | Tot: 17s357ms | Loss: 2.596 | Acc: 37.855% (8328/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      " [================================================================>]  Step: 74ms | Tot: 3m9s | Loss: 2.429 | Acc: 41.091% (36160/8800 2750/2750 50 ==========>.....................................................]  Step: 67ms | Tot: 34s148ms | Loss: 2.323 | Acc: 43.056% (6889/1600 500/2750 ============>....................................................]  Step: 66ms | Tot: 37s198ms | Loss: 2.333 | Acc: 42.856% (7474/1744 545/2750 ======================>..........................................]  Step: 70ms | Tot: 1m4s | Loss: 2.378 | Acc: 41.946% (12698/3027 946/2750 ========================>........................................]  Step: 71ms | Tot: 1m12s | Loss: 2.381 | Acc: 41.931% (14196/3385 1058/2750 =====================================>...........................]  Step: 66ms | Tot: 1m47s | Loss: 2.402 | Acc: 41.457% (20828/5024 1570/2750 =======================================>.........................]  Step: 65ms | Tot: 1m54s | Loss: 2.405 | Acc: 41.380% (22074/5334 1667/2750 ==========================================>......................]  Step: 66ms | Tot: 2m2s | Loss: 2.405 | Acc: 41.410% (23587/5696 1780/2750 ===========================================>.....................]  Step: 66ms | Tot: 2m6s | Loss: 2.408 | Acc: 41.333% (24390/5900 1844/2750 ========================================================>........]  Step: 67ms | Tot: 2m43s | Loss: 2.427 | Acc: 41.028% (31129/7587 2371/2750 =========================================================>.......]  Step: 67ms | Tot: 2m46s | Loss: 2.427 | Acc: 41.053% (31857/7760 2425/2750 \n",
      " [================================================================>]  Step: 23ms | Tot: 17s776ms | Loss: 2.600 | Acc: 38.159% (8395/2200 688/688 ...........................................................]  Step: 25ms | Tot: 1s11ms | Loss: 2.578 | Acc: 38.682% (458/118 37/688 ========>........................................................]  Step: 25ms | Tot: 2s349ms | Loss: 2.580 | Acc: 38.849% (1094/281 88/688 =============>...................................................]  Step: 25ms | Tot: 3s879ms | Loss: 2.584 | Acc: 38.486% (1835/476 149/688 ================>................................................]  Step: 25ms | Tot: 4s532ms | Loss: 2.591 | Acc: 38.536% (2158/560 175/688 =============================>...................................]  Step: 25ms | Tot: 7s972ms | Loss: 2.584 | Acc: 38.518% (3821/992 310/688 ===============================>.................................]  Step: 25ms | Tot: 8s577ms | Loss: 2.581 | Acc: 38.501% (4115/1068 334/688 ====================================>............................]  Step: 25ms | Tot: 9s928ms | Loss: 2.591 | Acc: 38.229% (4722/1235 386/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 10\n",
      " [================================================================>]  Step: 67ms | Tot: 3m12s | Loss: 2.384 | Acc: 42.105% (37052/8800 2750/2750 0 =>.............................................................]  Step: 68ms | Tot: 10s705ms | Loss: 2.252 | Acc: 44.869% (2125/473 148/2750 ===>.............................................................]  Step: 66ms | Tot: 11s783ms | Loss: 2.250 | Acc: 44.912% (2357/524 164/2750 ====>............................................................]  Step: 68ms | Tot: 13s735ms | Loss: 2.248 | Acc: 45.029% (2781/617 193/2750 =====>...........................................................]  Step: 66ms | Tot: 16s282ms | Loss: 2.248 | Acc: 44.886% (3318/739 231/2750 ===========>.....................................................]  Step: 66ms | Tot: 33s642ms | Loss: 2.298 | Acc: 43.743% (6649/1520 475/2750 ==========================>......................................]  Step: 70ms | Tot: 1m17s | Loss: 2.335 | Acc: 43.064% (15324/3558 1112/2750 =====================================>...........................]  Step: 66ms | Tot: 1m51s | Loss: 2.348 | Acc: 42.812% (21865/5107 1596/2750 =================================================>...............]  Step: 66ms | Tot: 2m25s | Loss: 2.367 | Acc: 42.471% (28364/6678 2087/2750 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 22ms | Tot: 17s534ms | Loss: 2.625 | Acc: 38.014% (8363/2200 688/688 ====================>..........................................]  Step: 25ms | Tot: 6s64ms | Loss: 2.614 | Acc: 37.868% (2884/761 238/688 =========================>.......................................]  Step: 25ms | Tot: 6s836ms | Loss: 2.624 | Acc: 37.920% (3252/857 268/688 =======================================>.........................]  Step: 26ms | Tot: 10s713ms | Loss: 2.626 | Acc: 37.954% (5101/1344 420/688 ==================================================>..............]  Step: 25ms | Tot: 13s731ms | Loss: 2.625 | Acc: 38.139% (6566/1721 538/688 \n",
      "\n",
      "Epoch: 11\n",
      " [================================================================>]  Step: 66ms | Tot: 3m13s | Loss: 2.331 | Acc: 43.236% (38048/8800 2750/2750 0 ========>.......................................................]  Step: 66ms | Tot: 29s487ms | Loss: 2.235 | Acc: 44.867% (5987/1334 417/2750 ==========>......................................................]  Step: 67ms | Tot: 31s47ms | Loss: 2.233 | Acc: 44.858% (6316/1408 440/2750 ===========================>.....................................]  Step: 66ms | Tot: 1m22s | Loss: 2.276 | Acc: 44.151% (16389/3712 1160/2750 ==============================>..................................]  Step: 68ms | Tot: 1m31s | Loss: 2.278 | Acc: 44.201% (18218/4121 1288/2750 ==================================>..............................]  Step: 67ms | Tot: 1m43s | Loss: 2.288 | Acc: 44.020% (20679/4697 1468/2750 ===========================================>.....................]  Step: 67ms | Tot: 2m9s | Loss: 2.306 | Acc: 43.656% (25537/5849 1828/2750 ================================================================>]  Step: 65ms | Tot: 3m13s | Loss: 2.331 | Acc: 43.238% (38022/8793 2748/2750 \n",
      " [================================================================>]  Step: 22ms | Tot: 17s118ms | Loss: 2.655 | Acc: 37.582% (8268/2200 688/688 ===========>...................................................]  Step: 25ms | Tot: 3s620ms | Loss: 2.686 | Acc: 37.001% (1705/460 144/688 \n",
      "\n",
      "Epoch: 12\n",
      " [================================================================>]  Step: 66ms | Tot: 3m13s | Loss: 2.293 | Acc: 43.924% (38653/8800 2750/2750 0 ======================================================>..........]  Step: 68ms | Tot: 2m42s | Loss: 2.284 | Acc: 44.052% (32535/7385 2308/2750 =======================================================>.........]  Step: 65ms | Tot: 2m45s | Loss: 2.285 | Acc: 44.050% (33013/7494 2342/2750 ==========================================================>......]  Step: 67ms | Tot: 2m54s | Loss: 2.286 | Acc: 44.061% (34882/7916 2474/2750 \n",
      " [================================================================>]  Step: 22ms | Tot: 17s405ms | Loss: 2.578 | Acc: 38.664% (8506/2200 688/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 13\n",
      " [================================================================>]  Step: 71ms | Tot: 3m23s | Loss: 2.246 | Acc: 44.910% (39521/8800 2750/2750 0  =========================================>.......................]  Step: 67ms | Tot: 2m12s | Loss: 2.210 | Acc: 45.484% (25544/5616 1755/2750 ===================================================>.............]  Step: 67ms | Tot: 2m42s | Loss: 2.231 | Acc: 45.197% (31486/6966 2177/2750 ========================================================>........]  Step: 67ms | Tot: 2m57s | Loss: 2.237 | Acc: 45.063% (34551/7667 2396/2750 ========================================================>........]  Step: 67ms | Tot: 2m58s | Loss: 2.237 | Acc: 45.073% (34746/7708 2409/2750 ===========================================================>.....]  Step: 67ms | Tot: 3m6s | Loss: 2.240 | Acc: 45.018% (36259/8054 2517/2750 \n",
      " [================================================================>]  Step: 23ms | Tot: 17s536ms | Loss: 2.577 | Acc: 39.327% (8652/2200 688/688 =====================================>...........................]  Step: 25ms | Tot: 10s140ms | Loss: 2.580 | Acc: 38.744% (4984/1286 402/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 14\n",
      " [================================================================>]  Step: 66ms | Tot: 3m13s | Loss: 2.212 | Acc: 45.633% (40157/8800 2750/2750 0 ===========>....................................................]  Step: 67ms | Tot: 37s858ms | Loss: 2.094 | Acc: 48.017% (8113/1689 528/2750 ==============>..................................................]  Step: 68ms | Tot: 42s815ms | Loss: 2.107 | Acc: 47.667% (9152/1920 600/2750 ================>................................................]  Step: 66ms | Tot: 50s89ms | Loss: 2.116 | Acc: 47.476% (10665/2246 702/2750 ==================================>..............................]  Step: 66ms | Tot: 1m44s | Loss: 2.171 | Acc: 46.280% (21859/4723 1476/2750 =======================================>.........................]  Step: 67ms | Tot: 1m59s | Loss: 2.179 | Acc: 46.199% (24940/5398 1687/2750 ===========================================>.....................]  Step: 66ms | Tot: 2m10s | Loss: 2.183 | Acc: 46.147% (27304/5916 1849/2750 ===================================================>.............]  Step: 66ms | Tot: 2m34s | Loss: 2.197 | Acc: 45.904% (32184/7011 2191/2750 ====================================================>............]  Step: 65ms | Tot: 2m36s | Loss: 2.196 | Acc: 45.905% (32611/7104 2220/2750 ============================================================>....]  Step: 66ms | Tot: 2m58s | Loss: 2.207 | Acc: 45.681% (37203/8144 2545/2750 ============================================================>....]  Step: 66ms | Tot: 2m59s | Loss: 2.207 | Acc: 45.680% (37348/8176 2555/2750 \n",
      " [================================================================>]  Step: 22ms | Tot: 17s49ms | Loss: 2.519 | Acc: 40.418% (8892/2200 688/688  ===============================================>.................]  Step: 24ms | Tot: 12s473ms | Loss: 2.531 | Acc: 40.251% (6466/1606 502/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 15\n",
      " [================================================================>]  Step: 76ms | Tot: 3m15s | Loss: 2.174 | Acc: 46.334% (40774/8800 2750/2750 0 ========>.......................................................]  Step: 68ms | Tot: 29s51ms | Loss: 2.046 | Acc: 49.468% (6696/1353 423/2750 ===========>.....................................................]  Step: 66ms | Tot: 32s885ms | Loss: 2.054 | Acc: 49.348% (7564/1532 479/2750 \n",
      " [================================================================>]  Step: 23ms | Tot: 17s877ms | Loss: 2.492 | Acc: 40.495% (8909/2200 688/688 ===========================================>.....................]  Step: 25ms | Tot: 12s16ms | Loss: 2.504 | Acc: 40.149% (5910/1472 460/688 =======================================================>.........]  Step: 34ms | Tot: 15s277ms | Loss: 2.499 | Acc: 40.321% (7561/1875 586/688 =========================================================>.......]  Step: 25ms | Tot: 15s828ms | Loss: 2.494 | Acc: 40.409% (7849/1942 607/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 66ms | Tot: 3m11s | Loss: 2.134 | Acc: 47.269% (41597/8800 2750/2750 0 ====>..........................................................]  Step: 66ms | Tot: 20s402ms | Loss: 1.985 | Acc: 50.462% (4699/931 291/2750 ===================>.............................................]  Step: 67ms | Tot: 56s604ms | Loss: 2.021 | Acc: 49.732% (12827/2579 806/2750 =================================>...............................]  Step: 69ms | Tot: 1m39s | Loss: 2.082 | Acc: 48.317% (21986/4550 1422/2750 ====================================>............................]  Step: 68ms | Tot: 1m48s | Loss: 2.087 | Acc: 48.190% (23933/4966 1552/2750 ======================================>..........................]  Step: 70ms | Tot: 1m54s | Loss: 2.091 | Acc: 48.095% (25117/5222 1632/2750 ===========================================>.....................]  Step: 68ms | Tot: 2m8s | Loss: 2.103 | Acc: 47.890% (28228/5894 1842/2750 ==================================================>..............]  Step: 67ms | Tot: 2m30s | Loss: 2.113 | Acc: 47.644% (32825/6889 2153/2750 =====================================================>...........]  Step: 66ms | Tot: 2m37s | Loss: 2.117 | Acc: 47.593% (34267/7200 2250/2750 ==========================================================>......]  Step: 70ms | Tot: 2m54s | Loss: 2.126 | Acc: 47.398% (37827/7980 2494/2750 \n",
      " [================================================================>]  Step: 22ms | Tot: 17s277ms | Loss: 2.500 | Acc: 41.182% (9060/2200 688/688 ======================================>..........................]  Step: 25ms | Tot: 10s299ms | Loss: 2.490 | Acc: 41.442% (5477/1321 413/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 17\n",
      " [================================================================>]  Step: 70ms | Tot: 3m14s | Loss: 2.099 | Acc: 47.803% (42067/8800 2750/2750 0 =======>........................................................]  Step: 68ms | Tot: 25s414ms | Loss: 1.941 | Acc: 50.757% (5831/1148 359/2750 ================>................................................]  Step: 66ms | Tot: 49s148ms | Loss: 1.996 | Acc: 49.573% (11041/2227 696/2750 ===============================================================>.]  Step: 67ms | Tot: 3m11s | Loss: 2.098 | Acc: 47.818% (41376/8652 2704/2750 \n",
      " [================================================================>]  Step: 30ms | Tot: 17s405ms | Loss: 2.495 | Acc: 41.127% (9048/2200 688/688 =====================================================>...........]  Step: 25ms | Tot: 14s416ms | Loss: 2.498 | Acc: 40.909% (7488/1830 572/688 \n",
      "\n",
      "Epoch: 18\n",
      " [================================================================>]  Step: 67ms | Tot: 3m15s | Loss: 2.064 | Acc: 48.701% (42857/8800 2750/2750 0 ==========>.....................................................]  Step: 67ms | Tot: 36s845ms | Loss: 1.912 | Acc: 51.798% (8238/1590 497/2750 ============>....................................................]  Step: 67ms | Tot: 39s583ms | Loss: 1.918 | Acc: 51.699% (8884/1718 537/2750 =============>...................................................]  Step: 67ms | Tot: 42s554ms | Loss: 1.924 | Acc: 51.552% (9568/1856 580/2750 ==============>..................................................]  Step: 66ms | Tot: 43s614ms | Loss: 1.927 | Acc: 51.523% (9810/1904 595/2750 ================>................................................]  Step: 67ms | Tot: 49s873ms | Loss: 1.941 | Acc: 51.265% (11270/2198 687/2750 ==============================================>..................]  Step: 66ms | Tot: 2m21s | Loss: 2.040 | Acc: 49.254% (31286/6352 1985/2750 \n",
      " [================================================================>]  Step: 22ms | Tot: 17s471ms | Loss: 2.455 | Acc: 42.009% (9242/2200 688/688 ==================>............................................]  Step: 25ms | Tot: 5s628ms | Loss: 2.483 | Acc: 41.370% (2886/697 218/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 19\n",
      " [================================================================>]  Step: 67ms | Tot: 3m12s | Loss: 2.037 | Acc: 49.170% (43270/8800 2750/2750 0 ==>............................................................]  Step: 67ms | Tot: 13s179ms | Loss: 1.822 | Acc: 54.438% (3275/601 188/2750 ====>............................................................]  Step: 71ms | Tot: 13s727ms | Loss: 1.827 | Acc: 54.305% (3406/627 196/2750 =====================>...........................................]  Step: 66ms | Tot: 1m2s | Loss: 1.929 | Acc: 51.626% (15017/2908 909/2750 ======================================>..........................]  Step: 67ms | Tot: 1m53s | Loss: 1.982 | Acc: 50.287% (26101/5190 1622/2750 ===============================================>.................]  Step: 66ms | Tot: 2m20s | Loss: 2.006 | Acc: 49.778% (31794/6387 1996/2750 \n",
      " [================================================================>]  Step: 22ms | Tot: 17s662ms | Loss: 2.469 | Acc: 41.905% (9219/2200 688/688 ==>..........................................................]  Step: 25ms | Tot: 1s861ms | Loss: 2.471 | Acc: 42.694% (970/227 71/688 =========================================>.......................]  Step: 26ms | Tot: 11s254ms | Loss: 2.465 | Acc: 42.120% (5890/1398 437/688 =============================================>...................]  Step: 25ms | Tot: 12s387ms | Loss: 2.467 | Acc: 42.070% (6462/1536 480/688 ==================================================>..............]  Step: 25ms | Tot: 13s720ms | Loss: 2.479 | Acc: 41.903% (7147/1705 533/688 \n",
      "\n",
      "Epoch: 20\n",
      " [================================================================>]  Step: 66ms | Tot: 3m13s | Loss: 2.000 | Acc: 49.974% (43977/8800 2750/2750 0 =>.............................................................]  Step: 68ms | Tot: 11s321ms | Loss: 1.827 | Acc: 54.413% (2577/473 148/2750 ====>............................................................]  Step: 67ms | Tot: 13s926ms | Loss: 1.850 | Acc: 53.646% (3193/595 186/2750 ==========>......................................................]  Step: 67ms | Tot: 31s962ms | Loss: 1.887 | Acc: 52.399% (7361/1404 439/2750 =============>...................................................]  Step: 67ms | Tot: 40s732ms | Loss: 1.891 | Acc: 52.201% (9488/1817 568/2750 =============>...................................................]  Step: 67ms | Tot: 41s2ms | Loss: 1.891 | Acc: 52.213% (9557/1830 572/2750 =================>...............................................]  Step: 67ms | Tot: 52s588ms | Loss: 1.903 | Acc: 52.047% (12308/2364 739/2750 =================>...............................................]  Step: 67ms | Tot: 53s945ms | Loss: 1.905 | Acc: 52.034% (12638/2428 759/2750 ======================>..........................................]  Step: 67ms | Tot: 1m6s | Loss: 1.920 | Acc: 51.712% (15522/3001 938/2750 ========================================>........................]  Step: 66ms | Tot: 2m1s | Loss: 1.958 | Acc: 50.727% (27823/5484 1714/2750 =======================================================>.........]  Step: 65ms | Tot: 2m45s | Loss: 1.983 | Acc: 50.290% (37673/7491 2341/2750 ==============================================================>..]  Step: 71ms | Tot: 3m6s | Loss: 1.995 | Acc: 50.048% (42313/8454 2642/2750 ================================================================>]  Step: 69ms | Tot: 3m11s | Loss: 1.999 | Acc: 49.992% (43433/8688 2715/2750 \n",
      " [================================================================>]  Step: 21ms | Tot: 17s452ms | Loss: 2.428 | Acc: 42.664% (9386/2200 688/688 ================================================>................]  Step: 25ms | Tot: 13s151ms | Loss: 2.428 | Acc: 42.676% (7033/1648 515/688 ===================================================>.............]  Step: 25ms | Tot: 13s850ms | Loss: 2.427 | Acc: 42.641% (7382/1731 541/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 65ms | Tot: 3m12s | Loss: 1.968 | Acc: 50.690% (44607/8800 2750/2750 0 =================>...............................................]  Step: 72ms | Tot: 52s9ms | Loss: 1.841 | Acc: 53.504% (12841/2400 750/2750 =====================>...........................................]  Step: 68ms | Tot: 1m4s | Loss: 1.867 | Acc: 52.878% (15652/2960 925/2750 =======================>.........................................]  Step: 68ms | Tot: 1m7s | Loss: 1.873 | Acc: 52.705% (16444/3120 975/2750 =================================>...............................]  Step: 66ms | Tot: 1m39s | Loss: 1.908 | Acc: 51.812% (23676/4569 1428/2750 =======================================================>.........]  Step: 72ms | Tot: 2m45s | Loss: 1.951 | Acc: 51.019% (38595/7564 2364/2750 \n",
      " [================================================================>]  Step: 23ms | Tot: 17s56ms | Loss: 2.428 | Acc: 42.486% (9347/2200 688/688  =================================================>...............]  Step: 25ms | Tot: 13s109ms | Loss: 2.420 | Acc: 42.689% (7240/1696 530/688 ===================================================>.............]  Step: 25ms | Tot: 13s387ms | Loss: 2.421 | Acc: 42.676% (7388/1731 541/688 ===================================================>.............]  Step: 25ms | Tot: 13s563ms | Loss: 2.420 | Acc: 42.644% (7478/1753 548/688 =======================================================>.........]  Step: 25ms | Tot: 14s466ms | Loss: 2.421 | Acc: 42.653% (7971/1868 584/688 \n",
      "\n",
      "Epoch: 22\n",
      " [================================================================>]  Step: 67ms | Tot: 3m12s | Loss: 1.932 | Acc: 51.470% (45294/8800 2750/2750 0 =========>......................................................]  Step: 65ms | Tot: 32s267ms | Loss: 1.753 | Acc: 55.689% (8233/1478 462/2750 ==========================>......................................]  Step: 66ms | Tot: 1m20s | Loss: 1.836 | Acc: 53.501% (19363/3619 1131/2750 ===========================================================>.....]  Step: 67ms | Tot: 2m56s | Loss: 1.923 | Acc: 51.644% (41530/8041 2513/2750 =============================================================>...]  Step: 64ms | Tot: 3m2s | Loss: 1.928 | Acc: 51.555% (42960/8332 2604/2750 ================================================================>]  Step: 68ms | Tot: 3m12s | Loss: 1.932 | Acc: 51.471% (45278/8796 2749/2750 \n",
      " [================================================================>]  Step: 25ms | Tot: 17s537ms | Loss: 2.407 | Acc: 42.545% (9360/2200 688/688 =========================================>.......................]  Step: 25ms | Tot: 11s318ms | Loss: 2.423 | Acc: 42.237% (5947/1408 440/688 =========================================>.......................]  Step: 25ms | Tot: 11s395ms | Loss: 2.424 | Acc: 42.205% (5983/1417 443/688 ================================================================>]  Step: 25ms | Tot: 17s457ms | Loss: 2.409 | Acc: 42.541% (9325/2192 685/688 \n",
      "\n",
      "Epoch: 23\n",
      " [================================================================>]  Step: 87ms | Tot: 3m10s | Loss: 1.910 | Acc: 51.756% (45545/8800 2750/2750 0  ............................................................]  Step: 66ms | Tot: 6s542ms | Loss: 1.672 | Acc: 56.875% (1729/304 95/2750 ==>..............................................................]  Step: 66ms | Tot: 6s677ms | Loss: 1.670 | Acc: 56.894% (1766/310 97/2750 ===>.............................................................]  Step: 68ms | Tot: 9s44ms | Loss: 1.675 | Acc: 56.700% (2395/422 132/2750 ===>.............................................................]  Step: 66ms | Tot: 11s173ms | Loss: 1.692 | Acc: 56.231% (2933/521 163/2750 ==========================================>......................]  Step: 65ms | Tot: 2m6s | Loss: 1.866 | Acc: 52.725% (30589/5801 1813/2750 ============================================>....................]  Step: 72ms | Tot: 2m10s | Loss: 1.872 | Acc: 52.613% (31467/5980 1869/2750 ====================================================>............]  Step: 66ms | Tot: 2m36s | Loss: 1.889 | Acc: 52.230% (37455/7171 2241/2750 \n",
      " [================================================================>]  Step: 25ms | Tot: 17s460ms | Loss: 2.384 | Acc: 43.150% (9493/2200 688/688 ===========>...................................................]  Step: 24ms | Tot: 3s758ms | Loss: 2.419 | Acc: 42.314% (2004/473 148/688 =======================================================>.........]  Step: 25ms | Tot: 14s920ms | Loss: 2.390 | Acc: 43.049% (8169/1897 593/688 \n",
      "Saving..\n",
      "\n",
      "Epoch: 24\n",
      " [==================================>..............................]  Step: 65ms | Tot: 1m43s | Loss: 1.806 | Acc: 53.972% (25475/4720 1475/2750   .............................................................]  Step: 67ms | Tot: 6s805ms | Loss: 1.700 | Acc: 56.917% (1621/284 89/2750 ====>............................................................]  Step: 69ms | Tot: 13s946ms | Loss: 1.663 | Acc: 57.493% (3514/611 191/2750 \r"
     ]
    }
   ],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "net = EfficientNetB0(num_classes=len(classes))\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "early_stopping = EarlyStopping(tolerance=5, min_delta=1)\n",
    "\n",
    "epoch = 0\n",
    "while(not(early_stopping.early_stop)):\n",
    "    train_loss = train(epoch) / len(train_dataloader)\n",
    "    validation_loss = test(epoch,'EffecientNet') / len(test_dataloader)\n",
    "    scheduler.step()\n",
    "    early_stopping(train_loss,validation_loss)\n",
    "    epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5CVBWjqtLeh"
   },
   "outputs": [],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "net = GoogLeNet(in_dim=in_dim,num_classes=len(classes))\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "early_stopping = EarlyStopping(tolerance=5, min_delta=1)\n",
    "\n",
    "epoch = 0\n",
    "while(not(early_stopping.early_stop)):\n",
    "    train_loss = train(epoch) / len(train_dataloader)\n",
    "    validation_loss = test(epoch,'GoogleNet') / len(test_dataloader)\n",
    "    scheduler.step()\n",
    "    early_stopping(train_loss,validation_loss)\n",
    "    epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeWVj_M1tL15"
   },
   "outputs": [],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "net = ResNeXt29_2x64d(in_dim=in_dim,num_classes=len(classes))\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "early_stopping = EarlyStopping(tolerance=5, min_delta=1)\n",
    "\n",
    "epoch = 0\n",
    "while(not(early_stopping.early_stop)):\n",
    "    train_loss = train(epoch) / len(train_dataloader)\n",
    "    validation_loss = test(epoch,'ResNext') / len(test_dataloader)\n",
    "    scheduler.step()\n",
    "    early_stopping(train_loss,validation_loss)\n",
    "    epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRoycSxZtMQQ"
   },
   "outputs": [],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "net = DLA(in_dim=in_dim,num_classes=len(classes))\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "early_stopping = EarlyStopping(tolerance=5, min_delta=1)\n",
    "\n",
    "epoch = 0\n",
    "while(not(early_stopping.early_stop)):\n",
    "    train_loss = train(epoch) / len(train_dataloader)\n",
    "    validation_loss = test(epoch,'DLA') / len(test_dataloader)\n",
    "    scheduler.step()\n",
    "    early_stopping(train_loss,validation_loss)\n",
    "    epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML_6.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "87c654493270e7c94b853c15fdeb109e0f0f2ae2d38a0b5f42ed947a584b7a2a"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "28b6c0c9832d41b68bdcf529a88e9823": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9142c841ae147339da419a60e489e62",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90afe6ae2ba641828ed88bbca6181d2d",
      "value": 170498071
     }
    },
    "48e26e6e1e01429d88a7a002590d804b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5981bee7f92940b5823aed3556f79b0e",
       "IPY_MODEL_28b6c0c9832d41b68bdcf529a88e9823",
       "IPY_MODEL_b4b284eeaee949968c2ed52c4737227b"
      ],
      "layout": "IPY_MODEL_d8fd8e6dbdc64aa383222481d2491ba8"
     }
    },
    "5981bee7f92940b5823aed3556f79b0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a75fb9d528b74555ac013f55593ddd6b",
      "placeholder": "",
      "style": "IPY_MODEL_705e9f12834c4fbca2f3fa8d304bad23",
      "value": "100%"
     }
    },
    "705e9f12834c4fbca2f3fa8d304bad23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90afe6ae2ba641828ed88bbca6181d2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "95bdb61438494daf8297d3481a759e64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a75fb9d528b74555ac013f55593ddd6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b482a3fbf44d4c08a3bae6e80bdc1fca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4b284eeaee949968c2ed52c4737227b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95bdb61438494daf8297d3481a759e64",
      "placeholder": "",
      "style": "IPY_MODEL_b482a3fbf44d4c08a3bae6e80bdc1fca",
      "value": " 170498071/170498071 [00:12&lt;00:00, 14044509.96it/s]"
     }
    },
    "c9142c841ae147339da419a60e489e62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8fd8e6dbdc64aa383222481d2491ba8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
